{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df90aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in e:\\anaconda\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in e:\\anaconda\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio~=0.17 in e:\\anaconda\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in e:\\anaconda\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in e:\\anaconda\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: sortedcontainers in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: outcome in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in e:\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in e:\\anaconda\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in e:\\anaconda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in e:\\anaconda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9931e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90decea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\ADMIN\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='wikitable sortable')\n",
    "\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    rank = cells[0].text.strip()\n",
    "    name = cells[1].text.strip()\n",
    "    artist = cells[2].text.strip()\n",
    "    upload_date = cells[4].text.strip()\n",
    "    views = cells[3].text.strip()\n",
    "\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "for i in range(len(rank_list)):\n",
    "    print(\"Rank:\", rank_list[i])\n",
    "    print(\"Name:\", name_list[i])\n",
    "    print(\"Artist:\", artist_list[i])\n",
    "    print(\"Upload date:\", upload_date_list[i])\n",
    "    print(\"Views:\", views_list[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94049351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bcci.tv/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "dropdown_menu = soup.find('li', class_='navigation__dropdown')\n",
    "fixtures_url = dropdown_menu.find('a', text='Fixtures').get('href')\n",
    "\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.content, 'html.parser')\n",
    "\n",
    "fixtures_container = fixtures_soup.find('section', class_='international-fixtures')\n",
    "\n",
    "fixtures = fixtures_container.find_all('div', class_='js-list')\n",
    "\n",
    "for fixture in fixtures:\n",
    "    match_title = fixture.find('strong').text.strip()\n",
    "    series = fixture.find('span', class_='u-unskewed-text').text.strip()\n",
    "    place = fixture.find('p', class_='fixture__additional-info').text.strip()\n",
    "    date = fixture.find('span', class_='fixture__datetime').text.strip()\n",
    "    time = fixture.find('span', class_='fixture__time').text.strip()\n",
    "\n",
    "    print(\"Match Title:\", match_title)\n",
    "    print(\"Series:\", series)\n",
    "    print(\"Place:\", place)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Time:\", time)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://statisticstimes.com/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "economy_url = soup.find('a', text='Economy').get('href')\n",
    "\n",
    "economy_response = requests.get(economy_url)\n",
    "\n",
    "economy_soup = BeautifulSoup(economy_response.content, 'html.parser')\n",
    "\n",
    "gdp_url = economy_soup.find('a', text='GDP of Indian states').get('href')\n",
    "\n",
    "gdp_response = requests.get(gdp_url)\n",
    "\n",
    "gdp_soup = BeautifulSoup(gdp_response.content, 'html.parser')\n",
    "\n",
    "table = gdp_soup.find('table', class_='display compact')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    rank = cells[0].text.strip()\n",
    "    state = cells[1].text.strip()\n",
    "    gdp_1819 = cells[2].text.strip()\n",
    "    gdp_1920 = cells[3].text.strip()\n",
    "    share_1819 = cells[4].text.strip()\n",
    "    gdp_billion = cells[5].text.strip()\n",
    "\n",
    "    print(\"Rank:\", rank)\n",
    "    print(\"State:\", state)\n",
    "    print(\"GSDP(18-19) - at current prices:\", gdp_1819)\n",
    "    print(\"GSDP(19-20) - at current prices:\", gdp_1920)\n",
    "    print(\"Share(18-19):\", share_1819)\n",
    "    print(\"GDP($ billion):\", gdp_billion)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc578e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://github.com/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "trending_url = url + 'trending'\n",
    "\n",
    "trending_response = requests.get(trending_url)\n",
    "\n",
    "trending_soup = BeautifulSoup(trending_response.content, 'html.parser')\n",
    "\n",
    "repositories = trending_soup.find_all('article', class_='Box-row')\n",
    "\n",
    "for repository in repositories:\n",
    "    repo_title = repository.find('h1').text.strip()\n",
    "    repo_desc = repository.find('p', class_='col-9').text.strip()\n",
    "    contributors_count = repository.find('a', class_='muted-link').text.strip()\n",
    "    language = repository.find('span', itemprop='programmingLanguage').text.strip()\n",
    "\n",
    "    print(\"Repository Title:\", repo_title)\n",
    "    print(\"Repository Description:\", repo_desc)\n",
    "    print(\"Contributors Count:\", contributors_count)\n",
    "    print(\"Language Used:\", language)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.billboard.com/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "charts_url = soup.find('a', text='Charts').get('href')\n",
    "\n",
    "charts_response = requests.get(charts_url)\n",
    "\n",
    "charts_soup = BeautifulSoup(charts_response.content, 'html.parser')\n",
    "\n",
    "hot100_url = charts_soup.find('a', text='Hot 100').get('href')\n",
    "\n",
    "hot100_response = requests.get(hot100_url)\n",
    "\n",
    "hot100_soup = BeautifulSoup(hot100_response.content, 'html.parser')\n",
    "\n",
    "songs_container = hot100_soup.find_all('li', class_='chart-list__element')\n",
    "\n",
    "for song in songs_container:\n",
    "    song_name = song.find('span', class_='chart-element__information__song').text.strip()\n",
    "    artist_name = song.find('span', class_='chart-element__information__artist').text.strip()\n",
    "    last_week_rank = song.find('span', class_='chart-element__meta text--last').text.strip()\n",
    "    peak_rank = song.find('span', class_='chart-element__meta text--peak').text.strip()\n",
    "    weeks_on_board = song.find('span', class_='chart-element__meta text--week').text.strip()\n",
    "\n",
    "    print(\"Song Name:\", song_name)\n",
    "    print(\"Artist Name:\", artist_name)\n",
    "    print(\"Last Week Rank:\", last_week_rank)\n",
    "    print(\"Peak Rank:\", peak_rank)\n",
    "    print(\"Weeks on Board:\", weeks_on_board)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey- compare'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='in-article sortable')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    book_name = cells[1].text.strip()\n",
    "    author_name = cells[2].text.strip()\n",
    "    volumes_sold = cells[3].text.strip()\n",
    "    publisher = cells[4].text.strip()\n",
    "    genre = cells[5].text.strip()\n",
    "\n",
    "    print(\"Book Name:\", book_name)\n",
    "    print(\"Author Name:\", author_name)\n",
    "    print(\"Volumes Sold:\", volumes_sold)\n",
    "    print(\"Publisher:\", publisher)\n",
    "    print(\"Genre:\", genre)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls095964455/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "series_container = soup.find('div', class_='lister-list')\n",
    "\n",
    "series_list = series_container.find_all('div', class_='lister-item')\n",
    "\n",
    "for series in series_list:\n",
    "    name = series.find('h3').find('a').text.strip()\n",
    "    year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    run_time = series.find('span', class_='runtime').text.strip()\n",
    "    ratings = series.find('div', class_='ipl-rating-star').find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    votes = series.find('p', class_='sort-num_votes-visible').find('span', attrs={'name': 'nv'}).text.strip()\n",
    "\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Year Span:\", year_span)\n",
    "    print(\"Genre:\", genre)\n",
    "    print(\"Run Time:\", run_time)\n",
    "    print(\"Ratings:\", ratings)\n",
    "    print(\"Votes:\", votes)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "show_all_url = url + 'ml/datasets.php'\n",
    "\n",
    "show_all_response = requests.get(show_all_url)\n",
    "\n",
    "show_all_soup = BeautifulSoup(show_all_response.content, 'html.parser')\n",
    "\n",
    "table = show_all_soup.find('table', class_='index')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    dataset_name = cells[0].text.strip()\n",
    "    data_type = cells[1].text.strip()\n",
    "    task = cells[2].text.strip()\n",
    "    attribute_type = cells[3].text.strip()\n",
    "    num_instances = cells[4].text.strip()\n",
    "    num_attributes = cells[5].text.strip()\n",
    "    year = cells[6].text.strip()\n",
    "\n",
    "    print(\"Dataset Name:\", dataset_name)\n",
    "    print(\"Data Type:\", data_type)\n",
    "    print(\"Task:\", task)\n",
    "    print(\"Attribute Type:\", attribute_type)\n",
    "    print(\"No. of Instances:\", num_instances)\n",
    "    print(\"No. of Attributes:\", num_attributes)\n",
    "    print(\"Year:\", year)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 9\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.naukri.com/hr-recruiters-consultants'\n",
    "\n",
    "response_home = requests.get(url)\n",
    "cookies = response_home.cookies\n",
    "session = response_home.headers['Set-Cookie'].split(';')[0]\n",
    "\n",
    "headers = {\n",
    "    'Referer': url,\n",
    "    'Cookie': session,\n",
    "}\n",
    "\n",
    "search_url = 'https://www.naukri.com/hr-recruiters-consultants'\n",
    "search_payload = {\n",
    "    'q': 'Data Science',\n",
    "}\n",
    "response_search = requests.get(search_url, headers=headers, cookies=cookies, params=search_payload)\n",
    "\n",
    "soup = BeautifulSoup(response_search.content, 'html.parser')\n",
    "\n",
    "recruiters_container = soup.find_all('article', class_='recSec')\n",
    "\n",
    "for recruiter in recruiters_container:\n",
    "    name = recruiter.find('a', class_='ellipsis').text.strip()\n",
    "    designation = recruiter.find('span', class_='designation').text.strip()\n",
    "    company = recruiter.find('div', class_='recInfo').find('a').text.strip()\n",
    "    skills = recruiter.find('div', class_='recInfo').find('div', class_='rec-suggestion').text.strip()\n",
    "    location = recruiter.find('div', class_='recInfo').find('span', class_='recLoc').text.strip()\n",
    "\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Designation:\", designation)\n",
    "    print(\"Company:\", company)\n",
    "    print(\"Skills they hire for:\", skills)\n",
    "    print(\"Location:\", location)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
