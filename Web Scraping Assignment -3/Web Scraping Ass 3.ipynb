{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(search_query):\n",
    "    base_url = 'https://www.amazon.in'\n",
    "    search_url = f'{base_url}/s?k={search_query}'\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_links = soup.find_all('a', class_='a-link-normal a-text-normal')\n",
    "\n",
    "        if product_links:\n",
    "            print(f\"Products under '{search_query}':\")\n",
    "            for link in product_links:\n",
    "                product_title = link.text.strip()\n",
    "                if product_title:\n",
    "                    print(product_title)\n",
    "        else:\n",
    "            print('No products found.')\n",
    "    else:\n",
    "        print('Failed to retrieve search results.')\n",
    "\n",
    "product = input('Enter the product to search: ')\n",
    "search_amazon_products(product)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a647793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scrape_product_details(keyword, num_pages):\n",
    "    base_url = 'https://www.amazon.in'\n",
    "    search_url = f'{base_url}/s?k={keyword}'\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(search_url)\n",
    "\n",
    "    product_urls = set()\n",
    "\n",
    "    for _ in range(num_pages):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        product_links = soup.find_all('a', class_='a-link-normal a-text-normal')\n",
    "\n",
    "        for link in product_links:\n",
    "            product_url = link.get('href')\n",
    "            if product_url.startswith('/'):\n",
    "                product_urls.add(base_url + product_url)\n",
    "\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-disabled']\")\n",
    "        if next_button:\n",
    "            break\n",
    "        else:\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_details = []\n",
    "    for url in product_urls:\n",
    "        details = scrape_product_info(url)\n",
    "        product_details.append(details)\n",
    "        \n",
    "\n",
    "    columns = ['Brand Name', 'Name of the Product', 'Price', 'Return/Exchange', 'Expected Delivery', 'Availability', 'Product URL']\n",
    "    df = pd.DataFrame(product_details, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_product_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    brand_name = get_product_info(soup, 'bylineInfo')\n",
    "    product_name = get_product_info(soup, 'product-title')\n",
    "    price = get_product_info(soup, 'priceblock_ourprice')\n",
    "    return_exchange = get_product_info(soup, 'icon-return-policy-2')\n",
    "    expected_delivery = get_product_info(soup, 'ddmDeliveryMessage')\n",
    "    availability = get_product_info(soup, 'availability')\n",
    "    product_url = url\n",
    "\n",
    "    details = [brand_name, product_name, price, return_exchange, expected_delivery, availability, product_url]\n",
    "    return details\n",
    "\n",
    "def get_product_info(soup, class_name):\n",
    "    element = soup.find('span', class_=class_name)\n",
    "    if element:\n",
    "        return element.text.strip()\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_pages = 3\n",
    "\n",
    "all_dataframes = []\n",
    "for keyword in keywords:\n",
    "    df = scrape_product_details(keyword, num_pages)\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_dataframes)\n",
    "combined_df.to_csv('product_details.csv', index=False)\n",
    "\n",
    "print('Product details saved to product_details.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://images.google.com\")\n",
    "\n",
    "    search_bar = driver.find_element(By.XPATH, \"//input[@name='q']\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    image_elements = driver.find_elements(By.XPATH, \"//img[@class='rg_i Q4LuWd']\")\n",
    "    num_scraped_images = min(num_images, len(image_elements))\n",
    "\n",
    "    image_urls = []\n",
    "    for i in range(num_scraped_images):\n",
    "        image_url = image_elements[i].get_attribute('src')\n",
    "        if image_url:\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images = 10\n",
    "\n",
    "for keyword in keywords:\n",
    "    image_urls = scrape_images(keyword, num_images)\n",
    "    for url in image_urls:\n",
    "        print(url)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_smartphone_details(keyword):\n",
    "    base_url = f'https://www.flipkart.com/search?q={keyword}&sid=tyy%2C4io&as=on&as-show=on&otracker=AS_QueryStore_HistoryAutoSuggest_1_9_na_na_ps&otracker1=AS_QueryStore_HistoryAutoSuggest_1_9_na_na_ps&as-pos=1&as-type=HISTORY&suggestionId=oneplus+nord%7CMobiles&as-searchtext={keyword}'\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('a', class_='_1fQZEK')\n",
    "\n",
    "        if products:\n",
    "            data = []\n",
    "            for product in products:\n",
    "                brand_name = product.find('div', class_='_4rR01T').text.strip()\n",
    "                smartphone_name = product.find('a', class_='IRpwTa').text.strip()\n",
    "                url = 'https://www.flipkart.com' + product['href']\n",
    "\n",
    "                details_response = requests.get(url)\n",
    "                if details_response.status_code == 200:\n",
    "                    details_soup = BeautifulSoup(details_response.content, 'html.parser')\n",
    "                    color = get_product_detail(details_soup, 'Color')\n",
    "                    ram = get_product_detail(details_soup, 'RAM')\n",
    "                    storage = get_product_detail(details_soup, 'Storage')\n",
    "                    primary_camera = get_product_detail(details_soup, 'Primary Camera')\n",
    "                    secondary_camera = get_product_detail(details_soup, 'Secondary Camera')\n",
    "                    display_size = get_product_detail(details_soup, 'Display Size')\n",
    "                    battery_capacity = get_product_detail(details_soup, 'Battery Capacity')\n",
    "                    price = get_product_detail(details_soup, '_30jeq3 _1_WHN1')\n",
    "                else:\n",
    "                    color = '-'\n",
    "                    ram = '-'\n",
    "                    storage = '-'\n",
    "                    primary_camera = '-'\n",
    "                    secondary_camera = '-'\n",
    "                    display_size = '-'\n",
    "                    battery_capacity = '-'\n",
    "                    price = '-'\n",
    "\n",
    "                data.append([brand_name, smartphone_name, color, ram, storage, primary_camera, secondary_camera, display_size, battery_capacity, price, url])\n",
    "\n",
    "            columns = ['Brand Name', 'Smartphone Name', 'Color', 'RAM', 'Storage (ROM)', 'Primary Camera', 'Secondary Camera', 'Display Size', 'Battery Capacity', 'Price', 'Product URL']\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            print('No products found.')\n",
    "    else:\n",
    "        print('Failed to retrieve search results.')\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_product_detail(soup, detail_name):\n",
    "    element = soup.find('li', attrs={'class': '_21Ahn-','data-tkid': f'product-details:{detail_name}'})\n",
    "    if element:\n",
    "        return element.find('span').text.strip()\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "keyword = input('Enter the smartphone to search: ')\n",
    "df = scrape_smartphone_details(keyword)\n",
    "\n",
    "if df is not None:\n",
    "    df.to_csv('smartphone_details.csv', index=False)\n",
    "    print('Smartphone details saved to smartphone_details.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_coordinates(city):\n",
    "    search_query = f'https://www.google.com/maps/search/{city}'\n",
    "    response = requests.get(search_query)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        map_div = soup.find('div', class_='BnJ3rd')\n",
    "        if map_div:\n",
    "            latitude = map_div.get('data-latitude')\n",
    "            longitude = map_div.get('data-longitude')\n",
    "            if latitude and longitude:\n",
    "                return latitude, longitude\n",
    "            else:\n",
    "                print(f'Coordinates not found for {city}.')\n",
    "        else:\n",
    "            print(f'Location not found for {city}.')\n",
    "    else:\n",
    "        print('Failed to retrieve coordinates.')\n",
    "\n",
    "    return None, None\n",
    "\n",
    "city = input('Enter the city name: ')\n",
    "latitude, longitude = scrape_coordinates(city)\n",
    "\n",
    "if latitude and longitude:\n",
    "    print(f'Coordinates for {city}:')\n",
    "    print('Latitude:', latitude)\n",
    "    print('Longitude:', longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer6\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_funding_deals():\n",
    "    url = 'https://trak.in/india-startup-funding-investment-2015/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', class_='tablepress tablepress-id-48')\n",
    "\n",
    "        if table:\n",
    "            data = []\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            for row in rows[1:]:\n",
    "                columns = row.find_all('td')\n",
    "                deal_date = columns[1].text.strip()\n",
    "                if 'Jan 2021' <= deal_date <= 'Mar 2021':\n",
    "                    startup_name = columns[2].text.strip()\n",
    "                    industry = columns[3].text.strip()\n",
    "                    sub_vertical = columns[4].text.strip()\n",
    "                    city = columns[5].text.strip()\n",
    "                    investor = columns[6].text.strip()\n",
    "                    investment_type = columns[7].text.strip()\n",
    "                    amount_in_usd = columns[8].text.strip()\n",
    "\n",
    "                    data.append([deal_date, startup_name, industry, sub_vertical, city, investor, investment_type, amount_in_usd])\n",
    "\n",
    "            columns = ['Deal Date', 'Startup Name', 'Industry', 'Sub-Vertical', 'City', 'Investor', 'Investment Type', 'Amount in USD']\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            print('Table not found on the page.')\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "\n",
    "    return None\n",
    "\n",
    "df = scrape_funding_deals()\n",
    "\n",
    "if df is not None:\n",
    "    df.to_csv('funding_deals.csv', index=False)\n",
    "    print('Funding deal details saved to funding_deals.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        if laptops:\n",
    "            data = []\n",
    "            for laptop in laptops:\n",
    "                name = laptop.text.strip()\n",
    "                details = laptop.find_next_sibling('ul').find_all('li')\n",
    "                specs = [detail.text.strip() for detail in details]\n",
    "                data.append([name] + specs)\n",
    "\n",
    "            columns = ['Laptop Name', 'Operating System', 'Display', 'Processor', 'Memory', 'Weight', 'Price']\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            print('Laptops not found on the page.')\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "\n",
    "    return None\n",
    "\n",
    "df = scrape_gaming_laptops()\n",
    "\n",
    "if df is not None:\n",
    "    df.to_csv('gaming_laptops.csv', index=False)\n",
    "    print('Gaming laptop details saved to gaming_laptops.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b28297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('div', class_='table-responsive')\n",
    "\n",
    "        if table:\n",
    "            data = []\n",
    "            rows = table.find_all('div', class_='rank')\n",
    "\n",
    "            for row in rows:\n",
    "                rank = row.text.strip()\n",
    "                name = row.find_next_sibling('div', class_='personName').text.strip()\n",
    "                net_worth = row.find_next_sibling('div', class_='netWorth').text.strip()\n",
    "                age = row.find_next_sibling('div', class_='age').text.strip()\n",
    "                citizenship = row.find_next_sibling('div', class_='countryOfCitizenship').text.strip()\n",
    "                source = row.find_next_sibling('div', class_='source').text.strip()\n",
    "                industry = row.find_next_sibling('div', class_='category').text.strip()\n",
    "\n",
    "                data.append([rank, name, net_worth, age, citizenship, source, industry])\n",
    "\n",
    "            columns = ['Rank', 'Name', 'Net Worth', 'Age', 'Citizenship', 'Source', 'Industry']\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            print('Table not found on the page.')\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "\n",
    "    return None\n",
    "\n",
    "df = scrape_billionaires()\n",
    "\n",
    "if df is not None:\n",
    "    df.to_csv('billionaires.csv', index=False)\n",
    "    print('Billionaires details saved to billionaires.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95428d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 9\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver_path = 'path_to_chromedriver'  # Replace with the actual path to the chromedriver executable\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "video_url = 'https://www.youtube.com/watch?v=your_video_id'  # Replace with the actual YouTube video URL or video ID\n",
    "driver.get(video_url)\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "comment_elements = driver.find_elements(By.CSS_SELECTOR, '#content-text')\n",
    "upvote_elements = driver.find_elements(By.CSS_SELECTOR, '#vote-count-middle')\n",
    "time_elements = driver.find_elements(By.CSS_SELECTOR, '#header-author > yt-formatted-string > a > span')\n",
    "\n",
    "comments = [element.text for element in comment_elements]\n",
    "upvotes = [element.text for element in upvote_elements]\n",
    "times = [element.get_attribute('innerText') for element in time_elements]\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 10\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.hostelworld.com/search?city=London&country=England'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "hostels = soup.find_all('div', class_='property-card')\n",
    "for hostel in hostels:\n",
    "    name = hostel.find('h2', class_='title').text.strip()\n",
    "    distance = hostel.find('span', class_='distance').text.strip()\n",
    "    ratings = hostel.find('div', class_='score orange').text.strip()\n",
    "    total_reviews = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "    overall_reviews = hostel.find('div', class_='keyword').text.strip()\n",
    "    privates_from_price = hostel.find('div', class_='price-col').text.strip()\n",
    "    dorms_from_price = hostel.find('div', class_='price-col').find_next('div', class_='price-col').text.strip()\n",
    "    facilities = ', '.join([fac.text.strip() for fac in hostel.find_all('span', class_='facilities-label')])\n",
    "    description = hostel.find('div', class_='rating-factors').text.strip()\n",
    "\n",
    "    print('Hostel Name:', name)\n",
    "    print('Distance from City Centre:', distance)\n",
    "    print('Ratings:', ratings)\n",
    "    print('Total Reviews:', total_reviews)\n",
    "    print('Overall Reviews:', overall_reviews)\n",
    "    print('Privates from Price:', privates_from_price)\n",
    "    print('Dorms from Price:', dorms_from_price)\n",
    "    print('Facilities:', facilities)\n",
    "    print('Description:', description)\n",
    "    print('-----------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
